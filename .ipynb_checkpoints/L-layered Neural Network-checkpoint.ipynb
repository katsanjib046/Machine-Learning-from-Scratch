{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Layered Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by initializing the weights and biases. The following function can be used to randomly initialize the weights and biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dim):\n",
    "    \"\"\"Initializes the parameters to a deep neural network.\n",
    "    Arguments:\n",
    "    n_layers-dim -- List containing the dimensions of each layers in the network.\n",
    "    epsilon -- a small number that is used to multiply the randomly generated weights to make them very small.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dictionary containing the parameters for different layers with index \"W1\", \"b1\", \"W2\", \"b2\", etc.\n",
    "                    Wl = weight matrix for layer l of shape (layers_dim[l-1], layers_dim[l])\n",
    "                    bl = bias matrix for layer l of shape (layers_dim[l], 1)\n",
    "    \"\"\"\n",
    "    L = len(layers_dim) # Gives the total number of layers\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1,L): # we don't need weight matrix for the first layer\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layers_dim[l], layers_dim[l-1])/ np.sqrt(layers_dims[l-1])# * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layers_dim[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid function.\n",
    "    Arguments:\n",
    "    Z -- An array Z\n",
    "    \n",
    "    Returns:\n",
    "    A -- activations\n",
    "    activation_cache -- the input Z is returned\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1 / (1+np.exp(-Z))\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the relu function.\n",
    "    Arguments:\n",
    "    Z -- An array Z\n",
    "    \n",
    "    Returns:\n",
    "    A -- activations\n",
    "    activation_cache -- the input Z is returned\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear(A, W, b):\n",
    "    \"\"\"Implements  linear forward propagation in a deep neural network\n",
    "    Arguments:\n",
    "    A -- Activation from the previous layer\n",
    "    W -- Weight matrix for this layer\n",
    "    b -- Bias matrix for this layer\n",
    "    \n",
    "    Returns:\n",
    "    Z --  pre-activation function for this layer\n",
    "    linear_cache -- storing the values of A, W and b for use in backward propagation\n",
    "    \"\"\"\n",
    "    Z = W@A + b\n",
    "    linear_cache = (A, W, b)\n",
    "    return Z, linear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation(A_prev,W,b, activation):\n",
    "    \"\"\"Implements the activation for this layer\n",
    "    Arguments:\n",
    "    Z -- pre-activation for this layer\n",
    "    activation -- either relu or sigmoid activation to be used\n",
    "    \n",
    "    Returns:\n",
    "    A -- Activation from this layer\n",
    "    activation_cache -- Activation cache to be used in backward propagation, which is just the Z\n",
    "    \"\"\"\n",
    "    if activation ==\"sigmoid\":\n",
    "        Z, linear_cache = forward_linear(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    if activation == \"relu\":\n",
    "        Z, linear_cache = forward_linear(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "        \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_combine(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the combine linear and activation forward movements.\n",
    "    Arguments:\n",
    "    X -- Input X\n",
    "    parameters -- parameters to be used, which is a dictionary\n",
    "    Returns:\n",
    "    AL -- The output activation\n",
    "    caches -- all caches combined from different layers for backward propagation\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    caches = []\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A,cache = forward_activation(A, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)],'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # for the last layer\n",
    "    AL, cache = forward_activation(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost given the output of last layer and the labels\n",
    "    Arguments:\n",
    "    AL -- Output of the last layer\n",
    "    Y -- Given labels for the input data\n",
    "    Returns:\n",
    "    cost -- a single real value giving cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = -(np.dot(Y,np.log(AL.T)) + np.dot((1-Y),np.log((1-AL).T)))/m\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_linear(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the linear part of backward propagation.\n",
    "    Arguments:\n",
    "    dZ -- the gradient of the cost wrt the linear output of the  next (l+1) layer\n",
    "    cache -- linear_cache from the forward propagation, contains (A_prev, W, b)\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1)\n",
    "    dW -- Gradient of the cost with respect to W (current layer l)\n",
    "    db -- Gradient of the cost with respect to b (current layer l)\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = dZ @ A_prev.T / m\n",
    "    db = np.sum(dZ,axis = 1, keepdims = True) / m\n",
    "    dA_prev = W.T @ dZ\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = backward_linear(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = backward_linear(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def backward_combine(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # initialization of backpropagation\n",
    "    dAL = - (np.divide(Y,AL) - np.divide(1-Y,1-AL))\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = backward_activation(dAL,current_cache,'sigmoid')\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_activation(dA_prev_temp,current_cache,'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "        \n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters['b' + str(l+1)] = parameters['b' + str(l+1)] - learning_rate * grads['db' + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Forward and Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []                         \n",
    "    parameters = initialize_parameters(layers_dims) #initializtaion\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = forward_combine(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        grads = backward_combine(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "test_dataset = h5py.File('test_catvnoncat.h5', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "train_x_orig = np.array(train_dataset['train_set_x'])\n",
    "train_y_orig = np.array(train_dataset['train_set_y'])\n",
    "train_y = train_y_orig.reshape((1,train_y_orig.shape[0]))\n",
    "test_x_orig = np.array(test_dataset['test_set_x'])\n",
    "test_y_orig = np.array(test_dataset['test_set_y'])\n",
    "test_y = test_y_orig.reshape((1,test_y_orig.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y =  0\n",
      "'y=1' is a cat and 'y=0' is a not cat!\n"
     ]
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 15\n",
    "plt.imshow(train_x_orig[index])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print (\"y = \", train_y[0,index])\n",
    "print (\"'y=1' is a cat and 'y=0' is a not cat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take few minutes!\n",
      "Cost after iteration 0: 0.7316167227495363\n",
      "Cost after iteration 100: 0.6080556222174327\n",
      "Cost after iteration 200: 0.5581503316238193\n",
      "Cost after iteration 300: 0.5060027173978532\n",
      "Cost after iteration 400: 0.47419244315991343\n",
      "Cost after iteration 500: 0.4156681183259093\n",
      "Cost after iteration 600: 0.33696577813578854\n",
      "Cost after iteration 700: 0.33872037489976625\n",
      "Cost after iteration 800: 0.20777106543076382\n",
      "Cost after iteration 900: 0.5840845420561495\n",
      "Cost after iteration 1000: 0.07632335463999707\n",
      "Cost after iteration 1100: 0.0463910785266822\n",
      "Cost after iteration 1200: 0.027926796626594707\n",
      "Cost after iteration 1300: 0.020385093631318567\n",
      "Cost after iteration 1400: 0.015841857446495137\n",
      "Cost after iteration 1500: 0.012674431810496531\n",
      "Cost after iteration 1600: 0.010346382248864066\n",
      "Cost after iteration 1700: 0.008658621702943634\n",
      "Cost after iteration 1800: 0.007383198810273449\n",
      "Cost after iteration 1900: 0.006396509379280558\n",
      "Cost after iteration 2000: 0.005602236972110348\n",
      "Cost after iteration 2100: 0.004962187295224417\n",
      "Cost after iteration 2200: 0.004440214167868946\n",
      "Cost after iteration 2300: 0.0040095650736654\n",
      "Cost after iteration 2400: 0.0036353922867194397\n",
      "Cost after iteration 2499: 0.00332370868365833\n"
     ]
    }
   ],
   "source": [
    "print(\"This might take few minutes!\")\n",
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is predicts the labels.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples\n",
    "    parameters -- parameters obtained from NN\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = forward_combine(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the training set:\n",
      "Accuracy: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "print(\"For the training set:\")\n",
    "pred_train = prediction(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the test set:\n",
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"For the test set:\")\n",
    "pred_train = prediction(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the next step is to use regularization to avoid overfitting. Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
